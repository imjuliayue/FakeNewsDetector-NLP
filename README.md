### Description
Found data on Kaggle
Cleaned data on Kaggle for better tokenization
- Use Aurebesh as a guide to do data splitting
- remove punctuation
- lemmatize
- remove filler words
- same capitalization
Visualized data (better understanding of data)
- word cloud
- word frequency
- title length frequency
- text length frequency

- Split data into train and test (80-20 split)

### Extracting Data Features

### Logistic Regression

To do:
- Visualize dates and subject
- Redo visualizations :(
- Use logistic regression
- Use SVM
- Use Ngrams
- build LLM or transformer
- Fine tune better models


